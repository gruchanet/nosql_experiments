[&#8810;](../exercise1.md) powrót

## Rozwiązanie <b>Zadania 1a</b>

| Database		| Import time	|
| ------------- |:-------------:|
| MongoDB		| 54m56s		|
| PostgreSQL 	| 1h08m5s		|

<br />

<h4 id="mongodb">MongoDB<h5>

<b>Polecenie</b>

	Measure-Command -Expression {mongoimport --db experiment --collection train --type csv --file .\Train.csv --headerline}
	
##### Ciekawostki
	
Przed wykonaniem zadania pojawił się problem z importem danych do Mongo.<br />
O co chodzi zorientowałem się dopiero po załadowaniu danych do bazy.<br />
Okazało się, że w bazie jest o dużo za dużo rekordów - bo aż <b>100mln</b>!<br />
Po skonfrontowaniu się z problemem i wstępnym przeglądzie pliku <i>"wyrwałem chwasta"</i> - zamieniłem znaki <b>CRLF</b> na <b>LF</b> i zaimportowałem dane z nowego pliku do bazy.<br />
Więcej info. pod [linkiem](http://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/forums/t/5594/number-of-train-test-cases-nested-csv-issues/29857#post29857)
	
##### Obserwacje

- <b>CPU</b> - podczas importu danych tylko <b>2</b> (<b>1</b> i <b>3</b> lub <b>1</b> i <b>4</b>) rdzenie spośród dostępnych <b>4</b> pracowały na wyższych obrotach. Drugi rdzeń pracował ze znacznie mniejszym obciążeniem. Całkowite użycie CPU nie przekroczyło 30%.

- <b>RAM</b> - zużycie pamięci nie przekraczało <b>5GB</b> (z maks. <b>6GB</b>). Dodatkowo dało się zaobserwować skoki podczas zwalniania pamięci, a częstotliwości zwalniania była niestała.

- <b>Dysk</b> - sytuacja podobna do tej z RAM-em - co jakiś czas dane tymczasowe były usuwane, aby odzyskać miejsce na dysku. 
Całkowite użycie miejsca na dysku: <b>10324.867MB</b>.

> Kilka screen-ów:

![Tutaj powinien być screen!!!](resources/1a_cpu.png "CPU")

![Gdzie jest screen?!](resources/1a_ram.png "RAM")

<br />

<h4 id="postgresql">PostgreSQL<h5>

Przed wykonaniem importu należało utworzyć tabelę train

	CREATE TABLE train(
		id INT PRIMARY KEY,
		title VARCHAR(255),
		body TEXT,
		tags VARCHAR(4095)
	);

<b>Polecenie</b>

	Measure-Command -Expression {.\psql -U test -d experiment -c "COPY train(Id, Title, Body, Tags) FROM '.\Train.csv' WITH DELIMITER ',' CSV HEADER"}
	
##### Ciekawostki

Aby przyspieszyć import danych do bazy Postgres-a zastosowałem się do punktów zamieszczonych w [dokumentacji](http://www.postgresql.org/docs/9.1/static/populate.html)
	
##### Obserwacje

Podczas importu do bazy PostgreSQL zużycie pamięci oraz procesora było znacznie mniejsze (nawet w szczytowym momencie).

<br />

#### KONKLUZJA:
<i>Dzięki "podrasowaniu" szybkości importowania danych dla bazy PostgreSQL, różnica czasowa wykonanych zadań między obiema bazami nie była zbyt wielka.</i>